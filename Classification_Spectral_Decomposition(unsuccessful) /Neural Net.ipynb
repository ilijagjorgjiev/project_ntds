{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import keras as ks \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from   tensorflow.keras import backend as K\n",
    "\n",
    "data_folder      = \"/nvme/drive_1/NTDS_Final/\"\n",
    "filtered_users   = pd.read_csv(data_folder+\"filtered_users.csv\",delimiter=',').values\n",
    "\n",
    "parallel_calls   = 12\n",
    "random_seed      = 1\n",
    "batch_len        = 1000\n",
    "num_epochs       = 2000\n",
    "sampled_nodes    = 2\n",
    "\n",
    "num_eigenvectors    = 100\n",
    "labels              = np.load(data_folder+\"new_filtering/labels.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** I. Setup Data Pipelines **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features       = np.load(data_folder+\"new_filtering/features.npy\")\n",
    "features       = (features - np.mean(features,axis=0)) / np.std(features,axis=0)\n",
    "\n",
    "dataset_train = []\n",
    "truth_train   = []\n",
    "for i,feat in enumerate(features[:50000]): \n",
    "\n",
    "    dataset_train.append(feat)\n",
    "    truth_train.append(labels[i,1])\n",
    "dataset_train = np.array(dataset_train)\n",
    "truth_train   = np.array(truth_train)\n",
    "\n",
    "dataset_test  = features[:50000]\n",
    "truth_test    = labels[50000:,1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** II. Create Neural Network Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    \n",
    "    print(y_pred)\n",
    "    \n",
    "    tp     = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    fp     = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn     = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    y_pred_clip = tf.clip_by_value(y_pred,0,1)\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred_clip, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred_clip), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred_clip, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred_clip), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    \n",
    "    return 1 - K.mean(f1)\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    input_1 = tf.keras.layers.Input(shape=[num_eigenvectors,],name='input_1') \n",
    "    \n",
    "    dense_1 = tf.keras.layers.BatchNormalization()(input_1)\n",
    "    dense_1 = tf.keras.layers.Dense(len(labels), activation='relu',\n",
    "                                    use_bias=True, name='dense_1')\\\n",
    "                                    (input_1) \n",
    "    dense_1 = tf.keras.layers.BatchNormalization()(dense_1)\n",
    "    dense_2 = tf.keras.layers.Dense(1, activation='sigmoid', \n",
    "                                    use_bias=True, name='dense_2')(dense_1) \n",
    "    \n",
    "    return tf.keras.Model(inputs=[input_1],outputs=[dense_2]) \n",
    "\n",
    "\n",
    "\n",
    "model  = build_model()\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.002),\n",
    "              #validation_data = data_iter_test,\n",
    "              loss=f1_loss,\n",
    "              metrics=['binary_crossentropy',f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III. Train and Test Network ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x = dataset_train, \n",
    "          y = truth_train,\n",
    "          epochs=20,\n",
    "          batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_acc = 0\n",
    "idx_acc   = 0\n",
    "for i,feat in enumerate(dataset_test):\n",
    "    #if truth_test[i] == 1:\n",
    "        y_pred    = np.rint(model.predict(np.array([feat])))\n",
    "        if y_pred >= 0.5:\n",
    "            y_pred =0\n",
    "        else:\n",
    "            y_pred =1\n",
    "        error_acc += np.abs(y_pred-truth_test[i])\n",
    "        idx_acc   += 1 \n",
    "        print(\"\\r\"+str(error_acc/idx_acc)+\" test error\", sep=' ', end='', flush=True) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files_test))\n",
    "print(len(files_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
